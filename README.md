<a name="top"></a>
# ğŸŒŸ Awesome Personalized Video Generation
[![Awesome](https://awesome.re/badge.svg)](https://awesome.re)
![PRs Welcome](https://img.shields.io/badge/PRs-Welcome-green) 

This repo is used for recording and tracking recent personalized video generation. ï¿¼ ï¿¼

PVGçš„ç›®æ ‡æ˜¯å®ç°å¯¹è§†é¢‘ç”Ÿæˆè¿‡ç¨‹çš„å…¨æ–¹ä½å®šåˆ¶ï¼Œè¿™ä¸ä»…åŒ…æ‹¬ä¸»ä½“èº«ä»½ï¼Œè¿˜æ¶µç›–äº†è‰ºæœ¯é£æ ¼ ã€èƒŒæ™¯åœºæ™¯ ã€è¿åŠ¨æ¨¡å¼ä¹ƒè‡³é•œå¤´è¯­è¨€ ã€‚


- [ğŸŒŸ Awesome Personalized Video Generation](#-awesome-personalized-video-generation)
  - [âš¡ Contributing](#-contributing)
  - [ğŸ“š Key Techniques \& Foundational Models](#-key-techniques--foundational-models)
    - [ğŸ–¼ï¸ Personalized Image Generation](#ï¸-personalized-image-generation)
    - [ğŸ‘¤ Subject Identity Learning](#-subject-identity-learning)
    - [ğŸ›ï¸ Multi-Modal Control Signal](#ï¸-multi-modal-control-signal)
    - [ğŸ“½ï¸ Foundation Video Generation Models](#ï¸-foundation-video-generation-models)
  - [ğŸŒ Open-Domain Personalized Video Generation Models](#-open-domain-personalized-video-generation-models)
    - [ğŸ¨ Subject-Driven Creation Models](#-subject-driven-creation-models)
      - [Test-time Fine-tuning](#test-time-fine-tuning)
      - [Pretrained Adaptation](#pretrained-adaptation)
    - [âœ‚ï¸ Video-Driven Editing Models](#ï¸-editing-models)
      - [Text-Guided Editing](#text-guided-editing)
      - [Subject Replacement/Addition](#subject-replacementaddition)
      - [Video Stylization](#video-stylization)
    - [ğŸ¥ Motion / Structure / Pose-Driven Models](#-motion--structure--pose-transfer)
    - [ğŸ”„ Multi-Modal Driven Models](#-multi-modal-driving-models)
  - [ğŸ§‘ Human-Domain Personalized Video Generation Models](#-human-domain-personalized-video-generation-models)
    - [ğŸ¨ ID-Driven Creation Models](#-id-driven-creation-models)
      - [Test-time Finetuning](#test-time-finetuning)
      - [Pretrained Adaptation](#pretrained-adaptation-1)
    - [âœ‚ï¸ Editing Models](#ï¸-editing-models-1)
      - [Text-Guided Editing](#text-guided-editing-1)
      - [ID Replacement/Addition](#id-replacementaddition)
    - [ğŸ•º Video Animation](#-video-animation)
      - [Audio-Driven Portrait Animation / Facial Animation](#portrait-animation--facial-animation)
      - [Pose-Driven Full-Body Animation](#full-body-animation)
  - [ğŸ’¼ Commercial Personalized Video Generation Models](#-commercial-s2v-models)
  - [ğŸ“ˆ Evaluation](#-evaluation)
    - [ğŸ“Š Personalized Video Generation Benchmarks](#-personalized-video-generation-benchmarks)
    - [ğŸ“‚ Personalized Video Generation Datasets](#-personalized-video-generation-datasets)
    - [ğŸ“ Key Evaluation Metrics](#-key-evaluation-metrics)
  - [ğŸ‘ Acknowledgement](#-acknowledgement)



## âš¡ Contributing

If you want to add your work to this list, please do not hesitate to email jhuang90@ur.rochester.edu or [pull requests]([https://github.com/inFaaa/Awesome-Personalized-Video-Generation/pulls](https://github.com/inFaaa/Awesome-Personalized-Video-Generation/pulls)).
Markdown format:

```markdown
* | [**Paper Title**] | Venue | Date | [[paper]](link) [[code]](link) |
```

## ğŸ“š Key Techniques & Foundational Models
### ğŸ–¼ï¸ Personalized Image Generation
- [DreamO](https://github.com/bytedance/DreamO)
- [RealCustom](https://github.com/bytedance/RealCustom)
- [UNO](https://github.com/bytedance/UNO)
- [InfiniteYou](https://github.com/bytedance/InfiniteYou/tree/main)
- [UniPortrait](https://github.com/junjiehe96/UniPortrait)
- [PuLID](https://github.com/ToTheBeginning/PuLID)
- [InstantID](https://github.com/instantX-research/InstantID)
- [PhotoMaker](https://github.com/TencentARC/PhotoMaker)
- [IP-Adapter](https://github.com/tencent-ailab/IP-Adapter)

### ğŸ‘¤ Subject Identity Learning
### ğŸ›ï¸ Multi-modal Control Signal
### ğŸ“½ï¸ Foundation Video Generation Models
- [Wan 2.1](https://github.com/Wan-Video/Wan2.1)
- [Step-Video](https://github.com/stepfun-ai/Step-Video-T2V)
- [HunyuanVideo](https://github.com/Tencent/HunyuanVideo)
- [LTX-Video](https://github.com/Lightricks/LTX-Video)
- [Mochi](https://github.com/genmoai/mochi)
- [CogVideo-X](https://github.com/THUDM/CogVideo)
- [Open-Sora-Plan](https://github.com/PKU-YuanGroup/Open-Sora-Plan)
- [Open-Sora](https://github.com/hpcaitech/Open-Sora)
- [Stable Video Diffusion](https://github.com/Stability-AI/generative-models)

## ğŸŒ Open-Domain Personalized Video Generation Models

### ğŸ¨ Subject-Driven Creation Models

#### Test-time Fine-tuning

#### Pretrained Adaptation

| Title                                                        | Venue     | Date             | Links                                                        |
| ------------------------------------------------------------ | --------- | ---------------- | ------------------------------------------------------------ |
| **PIA: Your Personalized Image Animator via Plug-and-Play Modules in Text-to-Image Models** | CVPRÂ 2024 | DecÂ 2023 (arXiv) | [Paper](https://arxiv.org/abs/2312.13964v3) â€“ [Project](https://pi-animator.github.io/)  - [Code](https://github.com/open-mmlab/PIA) |
| **VideoBooth: Diffusion-based Video Generation with Image Prompts** | CVPRÂ 2024 | DecÂ 2023 (arXiv) | [Paper](https://arxiv.org/abs/2312.00777) â€“ [Project](https://vchitect.github.io/VideoBooth-project/) â€“ [Code](https://github.com/vchitect/VideoBooth) |
| **CustomVideo: Customizing Text-to-Video Generation with Multiple Subjects** | arXiv     | Jan 18Â 2024      | [Paper](https://arxiv.org/pdf/2401.09962) â€“ [Project](https://kyfafyd.wang/projects/customvideo/) |
| **Movie Gen: A Cast of Media Foundation Models**             | arXiv     | OctÂ 17Â 2024      | [Paper](https://arxiv.org/abs/2410.13720) â€“ [Project](https://ai.meta.com/research/movie-gen/) |
| **VideoMaker: Zero-shot Customized Video Generation with the Inherent Force of Video Diffusion Models** | arXiv     | DecÂ 27Â 2024      | [Paper](https://arxiv.org/abs/2412.19645) â€“ [Project](https://wutao-cs.github.io) â€“ [Code](https://github.com/WuTao-CS/VideoMaker) |
| **Multi-subject Open-set Personalization in Video Generation** | CVPRÂ 2025 | JanÂ 2025 (arXiv) | [Paper](https://arxiv.org/abs/2501.06187) â€“ [Project](https://snap-research.github.io/open-set-video-personalization/) â€“ [Code](https://github.com/snap-research/open-set-video-personalization) |
| **Phantom: Subject-Consistent Video Generation via Cross-Modal Alignment** | arXiv     | FebÂ 16Â 2025      | [Paper](https://arxiv.org/abs/2502.11079) â€“ [Project](https://phantom-video.github.io/Phantom/) â€“ [Code](https://github.com/Phantom-video/Phantom) |
| **CINEMA: Coherent Multi-Subject Video Generation via MLLM-Based Guidance** | arXiv     | MarÂ 13Â 2025      | [Paper](https://arxiv.org/abs/2503.10391) |
| **VideoMage: Multi-Subject and Motion Customization of Text-to-Video Diffusion Models** | CVPR 2025     | MarÂ 13Â 2025      | [Paper](https://arxiv.org/abs/2503.21781) [Project](https://jasper0314-huang.github.io/videomage-customization/) |
| **SkyReels-A2: Compose Anything in Video Diffusion Transformers** | arXiv     | AprÂ 3Â 2025       | [Paper](https://arxiv.org/abs/2504.02436) â€“ [Project](https://skyworkai.github.io/SkyReels-A2/) â€“ [Code](https://github.com/SkyWorkAI/skyreels-a2) |
| **BridgeIV: Bridging Customized Image and Video Generation through Test-Time Autoregressive Identity Propagation** | arXiv     | MayÂ 11Â 2025      | [Paper](https://arxiv.org/pdf/2505.06985)                    |
| **MAGREF: Masked Guidance for Any-Reference Video Generation** | arXiv     | MayÂ 29Â 2025      | [Paper](https://arxiv.org/pdf/2505.23742) [Code](https://github.com/MAGREF-Video/MAGREF)                   |
| **AnyCharV: Bootstrap Controllable Character Video Generation with Fine-to-Coarse Guidance** | arXiv     | FebÂ 2025      | [Paper](https://arxiv.org/abs/2502.08189) â€“ [Code](https://github.com/AnyCharV/AnyCharV)                   |
| **Movie Weaver: Tuning-Free Multi-Concept Video Personalization with Anchored Prompts** | arXiv     | FebÂ 2025      | [Paper](https://arxiv.org/abs/2502.07802)                  |
| **ConceptMaster: Multi-Concept Video Customization on Diffusion Transformer Models Without Test-Time Tuning** | arXiv     | JanÂ 2025     | [Paper](https://arxiv.org/abs/2501.04698)                  |
| **Customcrafter: Customized Video Generation with Preserving Motion and Concept Composition Abilities** | arXiv     | FebÂ 2025      | [Paper](https://arxiv.org/abs/2408.13239) â€“ [Code](https://github.com/WuTao-CS/CustomCrafter)                   |

### âœ‚ï¸ Editing Models

#### Text-Guided Editing

#### Subject Replacement/Addition

| Title                                                        | Venue     | Date         | Links                                                        |
| ------------------------------------------------------------ | --------- | ------------ | ------------------------------------------------------------ |
| **Get In Video: Add Anything You Want to the Video** | arXiv | May 2025  | [Code](https://zhuangshaobin.github.io/GetInVideo-project/) â€“ [Paper](https://arxiv.org/abs/2503.06268) |

#### Video Stylization

| Title                                                        | Venue     | Date         | Links                                                        |
| ------------------------------------------------------------ | --------- | ------------ | ------------------------------------------------------------ |
| **Tune-A-Video: One-Shot Tuning of Image Diffusion Models for Text-to-Video Generation** | ICCV 2023 | Dec 22 2022  | [Code](https://github.com/showlab/Tune-A-Video) <br> [Paper](https://arxiv.org/abs/2212.11565) |
| **Structure and Content-Guided Video Synthesis with Diffusion Models** | ICCV 2023 | Feb 2023     | [Paper](https://openaccess.thecvf.com/content/ICCV2023/papers/Esser_Structure_and_Content-Guided_Video_Synthesis_with_Diffusion_Models_ICCV_2023_paper.pdf) |
| **Dreamix: Video Diffusion Models are General Video Editors** | arXiv     | FebÂ 2023     | [Paper](https://arxiv.org/abs/2302.01329) â€“ [Project](https://dreamix-video-editing.github.io/) |
| **Make-A-Protagonist: Generic Video Editing with Visual and Textual Clues** | arXiv     | May 15Â Â 2023 | [Paper](https://arxiv.org/pdf/2305.08850) â€“ [Code](https://github.com/HeliosZhao/Make-A-Protagonist) |
| **Cut-and-Paste: Subject-Driven Video Editing with Attention Control** | arXiv     | NovÂ 20Â 2023  | [Paper](https://arxiv.org/abs/2311.11697) â€“ [Code](https://github.com/ZrrSkywalker/Cut-And-Paste) |
| **DIVE: Taming DINO for Subject-Driven Video Editing**       | arXiv     | DecÂ 4Â 2024   | [Paper](https://arxiv.org/abs/2412.03347) â€“ [Project](https://dino-video-editing.github.io) â€“ [Code](https://github.com/OpenDriveLab/DIVE) |


### ğŸ¥ Motion / Structure / Pose Transfer
| Title                                                        | Venue     | Date         | Links                                                        |
| ------------------------------------------------------------ | --------- | ------------ | ------------------------------------------------------------ |
| **VideoComposer: Compositional Video Synthesis with Motion Controllability** | NeurIPSÂ 2023 | JunÂ 2023 (arXiv) | [Paper](https://arxiv.org/pdf/2306.02018) â€“ [Project](https://videocomposer.github.io/) - [Code](https://github.com/ali-vilab/videocomposer) |
| **DreamVideo: Composing Your Dream Videos with Customized Subject and Motion** | CVPRÂ 2024 | DecÂ 2023 (arXiv) | [Paper](https://arxiv.org/abs/2312.04433) â€“ [Project](https://vchitect.github.io/VideoBooth-project/) - [Code](https://github.com/Vchitect/VideoBooth) |
| **Customize-A-Video: One-Shot Motion Customization of Text-to-Video Diffusion Models** | ECCV2024  | Feb 2024     | â€” <br> [Paper](https://arxiv.org/abs/2402.14780) - [Project](https://ryx19th.github.io/customize-a-video/) - [Code](https://github.com/customize-a-video/customize-a-video) |
| **Subject-driven Video Generation via Disentangled Identity and Motion** | arXiv     | AprÂ 23Â 2025      | [Paper](https://arxiv.org/abs/2504.17816) â€“ [Code](https://github.com/carpedkm/disentangled-subject-to-vid) |
| **DualReal: Adaptive Joint Training for Lossless Identity-Motion Fusion in Video Customization** | arXiv     | MarÂ 4Â 2025       | [Paper](https://arxiv.org/abs/2505.02192) â€“ [Project](https://wenc-k.github.io/dualreal/) |
| **JointTuner: Appearance-Motion Adaptive Joint Training for Customized Video Generation** | arXiv     | MarÂ 31Â 2025      | [Paper](https://arxiv.org/abs/2503.23951) â€“ [Project](https://fdchen24.github.io/JointTuner-Website/) |
| **PolyVivid: Vivid Multi-Subject Video Generation with Cross-Modal Interaction and Enhancement** | arXiv     | JunÂ 9Â 2025      | [Paper](https://sjtuplayer.github.io/projects/PolyVivid/)                    |



### ğŸ”„ Multi-modal Driving Models 

| Title                                           | Venue | Date     | Links                                                        |
| ----------------------------------------------- | ----- | -------- | ------------------------------------------------------------ |
| **VACE: All-in-One Video Creation and Editing** | arxiv | Mar 2025 | [Code](https://github.com/ali-vilab/VACE) - [Project](https://ali-vilab.github.io/VACE-Page/) - [Paper](https://github.com/ali-vilab/VACE) |
| **HunyuanCustom: A Multimodal-Driven Architecture for Customized Video Generation** | arXiv     | MayÂ 8Â 2025       | [Paper](https://arxiv.org/pdf/2505.04512) â€“ [Project](https://hunyuancustom.github.io/) â€“ [Code](https://github.com/Tencent-Hunyuan/HunyuanCustom) |


## ğŸ§‘ Human-Domain Personalized Video Generation Models

### ğŸ¨ ID-Driven Creation Models

#### Test-time Finetuning

#### Pretrained Adaptation

| Title                                                        | Venue     | Date        | Links                                                        |
| ------------------------------------------------------------ | --------- | ----------- | ------------------------------------------------------------ |
| **Magic-Me: Identity-Specific Video Customized Diffusion**   | arXiv     | MarÂ 20Â 2024 | [Paper](https://arxiv.org/pdf/2402.09368) â€“ [Project](https://magic-me-webpage.github.io/) â€“ [Code](https://github.com/PKU-Alignment/ID-Animator) |
| **ID-Animator: Zero-Shot Identity-Preserving Human Video Generation** | arXiv     | AprÂ 23Â 2024 | [Paper](https://arxiv.org/abs/2404.15275) â€“ [Project](https://id-animator.github.io) â€“ [Code](https://github.com/Zhen-Dong/Magic-Me) |
| **ConsisID: Identity-Preserving Text-to-Video Generation by Frequency Decomposition** | CVPRÂ 2025 | NovÂ 26Â 2024 | [Paper](https://arxiv.org/pdf/2411.17383) â€“ [Code](https://github.com/PKU-YuanGroup/ConsisID) |
| **AnchorCrafter: Animate CyberAnchors Saling Your Products via Human-Object Interacting Video Generation** | arXiv | NovÂ 26Â 2024 | [Paper](https://arxiv.org/abs/2411.17440) â€“ [Code](https://github.com/cangcz/AnchorCrafter) |
| **Ingredients: Blending Custom Photos with Video Diffusion Transformers** | arXiv     | JanÂ 3Â 2025 | [Paper](https://arxiv.org/pdf/2501.01790?) â€“ [Code](https://github.com/feizc/Ingredients) |
| **EchoVideo: Identity-Preserving Human Video Generation by Multimodal Feature Fusion** | arXiv     | JanÂ 23Â 2025 | [Paper](https://arxiv.org/abs/2501.13452) â€“ [Code](https://github.com/JeremyWV/EchoVideo) |
| **FantasyID: Face Knowledge Enhanced ID-Preserving Video Generation** | arXiv     | FebÂ 25Â 2025 | [Paper](https://arxiv.org/abs/2502.13995) â€“ [Project](https://fantasy-amap.github.io/fantasy-id/) â€“ [Code](https://github.com/Fantasy-AMAP/fantasy-id) |
| **PersonalVideo: High ID-Fidelity Video Customization without Dynamic and Semantic Degradation** | arXiv     | MarÂ 16Â 2025 | [Paper](https://arxiv.org/pdf/2411.17048) â€“ [Project](https://personalvideo.github.io/) â€“[Code](https://github.com/EchoPluto/PersonalVideo) |
| **MagicID: Hybrid Preference Optimization for ID-Consistent and Dynamic-Preserved Video Customization** | arXiv     | MarÂ 16Â 2025 | [Paper](https://arxiv.org/abs/2503.12689) â€“ [Project](https://echopluto.github.io/MagicID-project/) â€“[Code](https://github.com/EchoPluto/MagicID) |
| **Concat-ID: Towards Universal Identity-Preserving Video Synthesis** | arXiv     | MarÂ 18Â 2025 | [Paper](https://arxiv.org/abs/2503.14151) â€“ [Code](https://github.com/ML-GSAI/Concat-ID) |
| **InterActHuman: Multi-Concept Human Animation with Layout-Aligned Audio Conditions** | arXiv     | JunÂ 11Â 2025 | [Paper](https://arxiv.org/pdf/2506.09984) â€“ [Project](https://zhenzhiwang.github.io/interacthuman/) |

### âœ‚ï¸ Editing Models
#### Text-Guided Editing

#### ID Replacement/Addition

#### Video Stylization

| Title                                                        | Venue | Date        | Links                                                        |
| ------------------------------------------------------------ | ----- | ----------- | ------------------------------------------------------------ |
| **IP-FaceDiff: Identity-Preserving Facial Video Editing with Diffusion** | arXiv | JanÂ 13Â 2025 | [Paper](https://arxiv.org/abs/2501.07530) â€“ [Code](https://github.com/ThoAce/IP-FaceDiff) |

### ğŸ•º Video Animation

#### Portrait Animation / Facial Animation

#### Full-Body Animation

## ğŸ’¼ Commercial Personalized Video Generation Models

- Pika
- Keling
- Veo
- Vidu
- Hailuo

## ğŸ“ˆ Evaluation

### ğŸ“Š Personalized Video Generation Benchmarks

| Title / Benchmark                                            | Venue                 | Date        | Links                                                        |
| ------------------------------------------------------------ | --------------------- | ----------- | ------------------------------------------------------------ |
| **ConsisID-Bench** â€“ 150 identities & 90 prompts (human-domain) | CVPR 2025 (Highlight) | Nov 2024    | [Project](https://pku-yuangroup.github.io/ConsisID) â€“ [Data](https://huggingface.co/datasets/PKU-YuanGroup/ConsisID-Bench) |
| **MSRVTT-Personalization** (Alchemist-Bench) â€“ Multi-subject personalization benchmark | CVPRÂ 2025             | JanÂ 2025    | [Paper](https://arxiv.org/abs/2501.06187) â€“ [Data/Code](https://github.com/snap-research/open-set-video-personalization) |
| **VACE-Benchmark**  â€“ VACE: All-in-One Video Creation and Editing | arXivÂ 2025            | MarÂ 2025    | [Paper](https://arxiv.org/abs/2503.07598) â€“ [Data/Code](https://huggingface.co/datasets/ali-vilab/VACE-Benchmark) |
| **A2 Bench** â€“ â€œElements-to-Videoâ€ evaluation benchmark for arbitrary subjects | arXiv  | AprÂ 2025    | [Paper](https://arxiv.org/abs/2504.02436) â€“ [Data/Code](https://github.com/SkyWorkAI/skyreels-a2) |
| **OpenS2V-Eval** â€“ Fine-grained S2V benchmark (180 prompts, real & synthetic) | arXiv                 | MayÂ 28Â 2025 | [Paper](https://arxiv.org/abs/2505.20292) â€“ [Project](https://pku-yuangroup.github.io/OpenS2V-Nexus) â€“ [Code](https://huggingface.co/spaces/BestWishYsh/OpenS2V-Eval) |

### ğŸ“‚ Personalized Video Generation Datasets

| Title / Dataset | Venue | Date        | Links                                                        |
| --------------- | ----- | ----------- | ------------------------------------------------------------ |
| **OpenS2V-5M**  | Arxiv | MayÂ 28Â 2025 | [Paper](https://arxiv.org/abs/2505.20292) â€“ [Project](https://pku-yuangroup.github.io/OpenS2V-Nexus) â€“ [Data](https://huggingface.co/datasets/BestWishYsh/OpenS2V-5M) |
| **ConsisID-Data**  | CVPR 2025 (Highlight) | MayÂ 28Â 2025 | [Paper](https://arxiv.org/abs/2411.17440) â€“ [Project](https://pku-yuangroup.github.io/ConsisID) â€“ [Data](https://huggingface.co/datasets/PKU-YuanGroup/ConsisID-Bench) |

### ğŸ“ Key Evaluation Metrics

## ğŸ‘ Acknowledgement

* [Awesome Personalization](https://github.com/zhangxulu1996/awesome-personalization)

* [Awesome-Personalized-Image-Generation](https://github.com/csyxwei/Awesome-Personalized-Image-Generation)

* [Awesome-Video-Diffusion](https://github.com/showlab/Awesome-Video-Diffusion)

* [Awesome-Controllable-Video-Generation](https://github.com/mayuelala/Awesome-Controllable-Video-Generation?tab=readme-ov-file#---personalized--motion)
  

  
